{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arabic-reshaper in c:\\users\\campos\\.conda\\envs\\tf\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: python-bidi in c:\\users\\campos\\.conda\\envs\\tf\\lib\\site-packages (0.4.2)\n",
      "Requirement already satisfied: six in c:\\users\\campos\\.conda\\envs\\tf\\lib\\site-packages (from python-bidi) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install arabic-reshaper python-bidi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Campos\\AppData\\Local\\Temp\\ipykernel_19008\\4045042623.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Campos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data_train = pd.read_csv('../Data/train_Arabic_tweets_positive_20190413.tsv', sep='\\t', header=None, names=['tweet'])\n",
    "negative_data_train = pd.read_csv('../Data/train_Arabic_tweets_negative_20190413.tsv', sep='\\t', header=None, names=['tweet'])\n",
    "positive_data_test = pd.read_csv('../Data/test_Arabic_tweets_positive_20190413.tsv', sep='\\t', header=None, names=['tweet'])\n",
    "negative_data_test = pd.read_csv('../Data/test_Arabic_tweets_positive_20190413.tsv', sep='\\t', header=None, names=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stop words and stemmer for Arabic\n",
    "stop_word = set(stopwords.words('arabic'))\n",
    "stemmer = ISRIStemmer()\n",
    "\n",
    "def preproces(text):\n",
    "    text  = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "    token = nltk.word_tokenize(text)\n",
    "    word =[stemmer.stem(word) for word in token if word not in stop_word]\n",
    "    return ' '.join(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...</td>\n",
       "      <td>تحل نود نقل دعء له، بحث فين قوة، انن مكسورون، قوة</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...</td>\n",
       "      <td>وفي نهي قىٰ معك حدإل رأىٰ جمل روح اماالمنبهر مظا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>من الخير نفسه 💛</td>\n",
       "      <td>خير نفس</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>#زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترض...</td>\n",
       "      <td>زلزلالملعبنصرنابيلعب علي همه رضى بغر قمه جرد س...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...</td>\n",
       "      <td>شيء وحد وصل علم سير ترى كان شجع درد ضد نصر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>السحب الليلة على الايفون .. رتويت للمرفقة وطبق...</td>\n",
       "      <td>سحب ليل ايف رتي رفق طبق شرط</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>😂 لابسة احمر ليه يا ست انتي ايه المناسبة 😂</td>\n",
       "      <td>لبس حمر ليه انت ايه نسب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>كلاام جمييل تستاهل(من احبه الله جعل محبته ف قل...</td>\n",
       "      <td>كلام جمييل تستاهلمن احب الل حبت قلب بشر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>- ألطف صورة ممكن تعبر عن رمضان 💙</td>\n",
       "      <td>لطف صور مكن عبر رمض</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>🌸 قال #الإمام_ابن_القيم -رحمه الله تعالى- : - ...</td>\n",
       "      <td>قال امامابنالقيم رحم الل على ير نعم الل أكل شر...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22761 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  \\\n",
       "pos  نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...   \n",
       "pos  وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...   \n",
       "pos                                    من الخير نفسه 💛   \n",
       "pos  #زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترض...   \n",
       "pos  الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...   \n",
       "..                                                 ...   \n",
       "pos  السحب الليلة على الايفون .. رتويت للمرفقة وطبق...   \n",
       "pos         😂 لابسة احمر ليه يا ست انتي ايه المناسبة 😂   \n",
       "pos  كلاام جمييل تستاهل(من احبه الله جعل محبته ف قل...   \n",
       "pos                   - ألطف صورة ممكن تعبر عن رمضان 💙   \n",
       "pos  🌸 قال #الإمام_ابن_القيم -رحمه الله تعالى- : - ...   \n",
       "\n",
       "                                       processed_tweet  \n",
       "pos  تحل نود نقل دعء له، بحث فين قوة، انن مكسورون، قوة  \n",
       "pos   وفي نهي قىٰ معك حدإل رأىٰ جمل روح اماالمنبهر مظا  \n",
       "pos                                            خير نفس  \n",
       "pos  زلزلالملعبنصرنابيلعب علي همه رضى بغر قمه جرد س...  \n",
       "pos         شيء وحد وصل علم سير ترى كان شجع درد ضد نصر  \n",
       "..                                                 ...  \n",
       "pos                        سحب ليل ايف رتي رفق طبق شرط  \n",
       "pos                            لبس حمر ليه انت ايه نسب  \n",
       "pos            كلام جمييل تستاهلمن احب الل حبت قلب بشر  \n",
       "pos                                لطف صور مكن عبر رمض  \n",
       "pos  قال امامابنالقيم رحم الل على ير نعم الل أكل شر...  \n",
       "\n",
       "[22761 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_data_train['processed_tweet'] = positive_data_train['tweet'].apply(preproces)\n",
    "positive_data_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_data_train['processed_tweet'] = negative_data_train['tweet'].apply(preproces)\n",
    "positive_data_test['processed_tweet'] = negative_data_test['tweet'].apply(preproces)\n",
    "negative_data_test['processed_tweet'] = negative_data_test['tweet'].apply(preproces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data_train['label'] = 1\n",
    "negative_data_train['label'] = 0\n",
    "positive_data_test['label'] = 1\n",
    "negative_data_test['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...</td>\n",
       "      <td>تحل نود نقل دعء له، بحث فين قوة، انن مكسورون، قوة</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...</td>\n",
       "      <td>وفي نهي قىٰ معك حدإل رأىٰ جمل روح اماالمنبهر مظا</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>من الخير نفسه 💛</td>\n",
       "      <td>خير نفس</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترض...</td>\n",
       "      <td>زلزلالملعبنصرنابيلعب علي همه رضى بغر قمه جرد س...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...</td>\n",
       "      <td>شيء وحد وصل علم سير ترى كان شجع درد ضد نصر</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45270</th>\n",
       "      <td>كيف ترى أورانوس لو كان يقع مكان القمر ؟ 💙💙 كوك...</td>\n",
       "      <td>ترى رنس يقع كان قمر ؟ كوكب شمس بلغ قطر كرة جوف...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45271</th>\n",
       "      <td>احسدك على الايم 💔</td>\n",
       "      <td>حسد ايم</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45272</th>\n",
       "      <td>لأول مرة ما بنكون سوا 💔</td>\n",
       "      <td>لأل مرة بنك سوا</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45273</th>\n",
       "      <td>بقله ليش يا واطي 🤔</td>\n",
       "      <td>بقل ليش وطي</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45274</th>\n",
       "      <td>قد طال صبري في النوى إذ تركتني كئيبا ؛ غريبا ب...</td>\n",
       "      <td>طال صبر نوى ترك كئب ؛ غرب باك توجع مهج يرح قلب...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45275 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  \\\n",
       "0      نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...   \n",
       "1      وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...   \n",
       "2                                        من الخير نفسه 💛   \n",
       "3      #زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترض...   \n",
       "4      الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...   \n",
       "...                                                  ...   \n",
       "45270  كيف ترى أورانوس لو كان يقع مكان القمر ؟ 💙💙 كوك...   \n",
       "45271                                  احسدك على الايم 💔   \n",
       "45272                            لأول مرة ما بنكون سوا 💔   \n",
       "45273                                 بقله ليش يا واطي 🤔   \n",
       "45274  قد طال صبري في النوى إذ تركتني كئيبا ؛ غريبا ب...   \n",
       "\n",
       "                                         processed_tweet  label  \n",
       "0      تحل نود نقل دعء له، بحث فين قوة، انن مكسورون، قوة      1  \n",
       "1       وفي نهي قىٰ معك حدإل رأىٰ جمل روح اماالمنبهر مظا      1  \n",
       "2                                                خير نفس      1  \n",
       "3      زلزلالملعبنصرنابيلعب علي همه رضى بغر قمه جرد س...      1  \n",
       "4             شيء وحد وصل علم سير ترى كان شجع درد ضد نصر      1  \n",
       "...                                                  ...    ...  \n",
       "45270  ترى رنس يقع كان قمر ؟ كوكب شمس بلغ قطر كرة جوف...      0  \n",
       "45271                                            حسد ايم      0  \n",
       "45272                                    لأل مرة بنك سوا      0  \n",
       "45273                                        بقل ليش وطي      0  \n",
       "45274  طال صبر نوى ترك كئب ؛ غرب باك توجع مهج يرح قلب...      0  \n",
       "\n",
       "[45275 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.concat([positive_data_train, negative_data_train], ignore_index=True)\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#الهلال_الاهلي فوز هلالي مهم الحمد لله 💙 زوران...</td>\n",
       "      <td>هلالالاهلي فوز هلل مهم حمد لله زور سلم برا بدل...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>صباحك خيرات ومسرات 🌸</td>\n",
       "      <td>صبح خير مسر</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#تأمل قال الله ﷻ :- _*​﴿بواد غير ذي زرع ﴾*_ 💫💫...</td>\n",
       "      <td>أمل قال الل بود زرع ومع هتف دعء رزق ثمر مهماكا...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>😂😂 يا جدعان الرجاله اللي فوق ال دول خطر ع تويت...</td>\n",
       "      <td>جدع رجل الل ال دول خطر يتر ورب مش سلب يوم دخل ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>رساله صباحيه : 💛 اللهم اسألك التوفيق في جميع ا...</td>\n",
       "      <td>رسل صبح لهم سأل وفق امر كتب ردس ولد جمع وتى سل...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11499</th>\n",
       "      <td>ربي اغفر لي و لوالدي و لأحبتي و للمؤمنين و الم...</td>\n",
       "      <td>ربي غفر للد أحب ؤمن ؤمن سلم سلم حيء نهم امو</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11500</th>\n",
       "      <td>ربي يسعدنا وياكم 💛</td>\n",
       "      <td>ربي سعد ويا</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11501</th>\n",
       "      <td>يتحدثون عن اخلاق حسين ونجوم فرقهم نهاياتهم الر...</td>\n",
       "      <td>حدث خلق حسن نجم فرق نهي ريض ليم خجل ختلف تفق ح...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11502</th>\n",
       "      <td>صباحكم احتفالية لم تكتمل، وصاحب الاحتفاليه ماك...</td>\n",
       "      <td>صبح احتفالية كتمل، صحب احتفاليه اكمل برا برضوه...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11503</th>\n",
       "      <td>قلت لكم سابقا المعيوف عندما تحتاجه لا يخذلك 💙</td>\n",
       "      <td>قلت سبق عيف عند حاج خذل</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11504 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  \\\n",
       "0      #الهلال_الاهلي فوز هلالي مهم الحمد لله 💙 زوران...   \n",
       "1                                   صباحك خيرات ومسرات 🌸   \n",
       "2      #تأمل قال الله ﷻ :- _*​﴿بواد غير ذي زرع ﴾*_ 💫💫...   \n",
       "3      😂😂 يا جدعان الرجاله اللي فوق ال دول خطر ع تويت...   \n",
       "4      رساله صباحيه : 💛 اللهم اسألك التوفيق في جميع ا...   \n",
       "...                                                  ...   \n",
       "11499  ربي اغفر لي و لوالدي و لأحبتي و للمؤمنين و الم...   \n",
       "11500                                 ربي يسعدنا وياكم 💛   \n",
       "11501  يتحدثون عن اخلاق حسين ونجوم فرقهم نهاياتهم الر...   \n",
       "11502  صباحكم احتفالية لم تكتمل، وصاحب الاحتفاليه ماك...   \n",
       "11503      قلت لكم سابقا المعيوف عندما تحتاجه لا يخذلك 💙   \n",
       "\n",
       "                                         processed_tweet  label  \n",
       "0      هلالالاهلي فوز هلل مهم حمد لله زور سلم برا بدل...      1  \n",
       "1                                            صبح خير مسر      1  \n",
       "2      أمل قال الل بود زرع ومع هتف دعء رزق ثمر مهماكا...      1  \n",
       "3      جدع رجل الل ال دول خطر يتر ورب مش سلب يوم دخل ...      1  \n",
       "4      رسل صبح لهم سأل وفق امر كتب ردس ولد جمع وتى سل...      1  \n",
       "...                                                  ...    ...  \n",
       "11499        ربي غفر للد أحب ؤمن ؤمن سلم سلم حيء نهم امو      0  \n",
       "11500                                        ربي سعد ويا      0  \n",
       "11501  حدث خلق حسن نجم فرق نهي ريض ليم خجل ختلف تفق ح...      0  \n",
       "11502  صبح احتفالية كتمل، صحب احتفاليه اكمل برا برضوه...      0  \n",
       "11503                            قلت سبق عيف عند حاج خذل      0  \n",
       "\n",
       "[11504 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.concat([positive_data_test, negative_data_test], ignore_index=True)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vector = TfidfVectorizer(max_features=5000)\n",
    "x_train = vector.fit_transform(train_data['processed_tweet']).toarray()\n",
    "y_train = train_data['label'].values\n",
    "\n",
    "x_test = vector.transform(test_data['processed_tweet']).toarray()\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45275, 5000), (45275,), (11504, 5000), (11504,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build MOdels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Traditional Machine Learning Alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "RF_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "RF_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = RF_model.predict(x_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = classification_report(y_test, y_pred,output_dict=True)\n",
    "report = pd.DataFrame(class_report).transpose()\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.\tDeep learning technique (RNN, LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Campos\\.conda\\envs\\tf\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokanizer = Tokenizer(num_words=5000)\n",
    "tokanizer.fit_on_texts(train_data['processed_tweet'])\n",
    "\n",
    "x_train_seq = tokanizer.texts_to_sequences(x_train['processed_tweet'])\n",
    "x_test_seq = tokenizer.texts_to_sequences(test_data['processed_tweet'])\n",
    "x_valid_seq = tokanizer.texts_to_sequences(x_valid['processed_tweet'])\n",
    "\n",
    "x_train_pad = pad_sequences(x_train_seq, maxlen=200)\n",
    "x_test_pad = pad_sequences(x_test_seq, maxlen=200)\n",
    "x_valid_pad = pad_sequences(x_valid_seq, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128, input_length=200))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimalizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_pad, y_train, epoch=100, batch_size=32,validation_data+(x_valid_pad, y_valid) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model.evalute(x_test_pad,y_test)\n",
    "print('Accuracy:' accuracy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.\tTransformers using HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = encode_reviews(train_data['processed_tweet'].tolist())\n",
    "test_encodings = encode_reviews(test_data['processed_tweet'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.convert_to_tensor(y_train)\n",
    "test_labels = tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_encodings['input_ids'], train_labels, epochs=2, batch_size=8, validation_data=(test_encodings['input_ids'], test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model.evaluate(test_encodings['input_ids'], test_labels)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
